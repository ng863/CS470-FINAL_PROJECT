{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "1. load csv file (panda, numpy)\n",
        "2. split dataset. Example code:()\n",
        "   ```\n",
        "   random.shuffle(data) # change if you are using pandas dataframe\n",
        "   training = data[:int(len(data)*0.8)]\n",
        "   test = data[int(len(data)*0.8):]\n",
        "\n",
        "   fold5 = KFold(5) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "   for train_idx, val_idx in fold5.split(training):\n",
        "      sub_val = training[val_idx]\n",
        "      sub_train = training[train_idx]\n",
        "      clf = model(sub_train, sub_val, ...) # training the model, and evaluate it on validation dataset\n",
        "      performance(clf, test) # test the model on test dataset\n",
        "   ```"
      ],
      "metadata": {
        "id": "uxgBX0YXu1du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive bayes\n",
        "\n",
        "1. model learning:\n",
        "\n",
        "   Note:\n",
        "\n",
        "   features: remove attributes that is not related to word (the last four attributes)\n",
        "\n",
        "   labels: the last column\n",
        "\n",
        "   count P(c) -> how many samples are positive, and how many are negtive\n",
        "\n",
        "   if freq_word>0, then this word exists. You could use this to calculate P(a|c) -> for each class, what is the prob of each word\n",
        "\n",
        "   remember to use laplace smoothing.\n",
        "\n",
        "2. model evaluation (on val dataset -> performance(model, val)):\n",
        "   \n",
        "   for each new sample, $\\prod{P(a|c)}P(c)$ if word is in the email(freq_word > 0); and find the maximum class\n",
        "   \n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "FXyRfd35yRPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "!ls\n",
        "%cd /content/drive/MyDrive/ColabNotebooks/\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb-Y67S1Tx-3",
        "outputId": "65801912-58b8-4001-c272-75eb810d1842"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Mounted at /content/drive\n",
            "drive  sample_data\n",
            "/content/drive/MyDrive/ColabNotebooks\n",
            "'Final Project (dont do show spam).ipynb'  'Final Project.ipynb'   spambase.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/spambase.csv\")\n",
        "\n",
        "data = data.iloc[:, :-4]\n",
        "\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "split_index = int(len(data) * 0.8)\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "def count_p_c(labels):\n",
        "    return (labels == 1).sum() / len(labels), (labels == 0).sum() / len(labels)\n",
        "\n",
        "def naive_bayes_train(training_data):\n",
        "    p_spam, p_not_spam = count_p_c(training_data.iloc[:, -1])\n",
        "\n",
        "    prob_spam = []\n",
        "    prob_not_spam = []\n",
        "    for i in range(len(training_data.columns) - 1):\n",
        "        spam_count = (training_data[training_data.iloc[:, -1] == 1].iloc[:, i] > 0).sum()\n",
        "        not_spam_count = (training_data[training_data.iloc[:, -1] == 0].iloc[:, i] > 0).sum()\n",
        "\n",
        "        if spam_count == 0 or not_spam_count == 0:\n",
        "            prob_spam.append(0)\n",
        "            prob_not_spam.append(0)\n",
        "        else:\n",
        "            prob_spam.append(spam_count / len(training_data[training_data.iloc[:, -1] == 1]))\n",
        "            prob_not_spam.append(not_spam_count / len(training_data[training_data.iloc[:, -1] == 0]))\n",
        "\n",
        "    return p_spam, p_not_spam, prob_spam, prob_not_spam\n",
        "\n",
        "def classify_email(email, p_spam, p_not_spam, prob_spam, prob_not_spam):\n",
        "    prob_spam_given_email = p_spam\n",
        "    prob_not_spam_given_email = p_not_spam\n",
        "    for i in range(len(email)):\n",
        "        if email[i] > 0:\n",
        "            prob_spam_given_email *= prob_spam[i]\n",
        "            prob_not_spam_given_email *= prob_not_spam[i]\n",
        "    return 1 if prob_spam_given_email > prob_not_spam_given_email else 0\n",
        "\n",
        "def evaluate_model(test_data, p_spam, p_not_spam, prob_spam, prob_not_spam):\n",
        "    correct = 0\n",
        "    for i in range(len(test_data)):\n",
        "        prediction = classify_email(test_data.iloc[i, :-1], p_spam, p_not_spam, prob_spam, prob_not_spam)\n",
        "        if prediction == test_data.iloc[i, -1]:\n",
        "            correct += 1\n",
        "    return correct / len(test_data)\n",
        "\n",
        "fold5 = 5\n",
        "kf = KFold(n_splits=fold5)\n",
        "\n",
        "train_accuracies = []\n",
        "for train_idx, val_idx in kf.split(X_train):\n",
        "    sub_val_X, sub_val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
        "    sub_train_X, sub_train_y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n",
        "\n",
        "    p_spam, p_not_spam, prob_spam, prob_not_spam = naive_bayes_train(pd.concat([sub_train_X, sub_train_y], axis=1))\n",
        "    accuracy = evaluate_model(pd.concat([sub_val_X, sub_val_y], axis=1), p_spam, p_not_spam, prob_spam, prob_not_spam)\n",
        "\n",
        "    train_accuracies.append(accuracy)\n",
        "\n",
        "p_spam, p_not_spam, prob_spam, prob_not_spam = naive_bayes_train(pd.concat([X_train, y_train], axis=1))\n",
        "test_accuracy = evaluate_model(pd.concat([X_test, y_test], axis=1), p_spam, p_not_spam, prob_spam, prob_not_spam)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Average Train Accuracy:\", np.mean(train_accuracies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccEYfLL8TU_J",
        "outputId": "e37713d1-d69e-4d95-9846-add5d4c29277"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8610206297502715\n",
            "Average Train Accuracy: 0.8309782608695653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN\n",
        "1. model learning: None\n",
        "\n",
        "2. model evaluation(on val dataset): You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "\n",
        "   ```\n",
        "   Note:\n",
        "   parallel programing\n",
        "   numpy.cos() to calcuate the similarity\n",
        "   ```"
      ],
      "metadata": {
        "id": "5jRvHTlW0DYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def cosine_similarity(x1, x2):\n",
        "    return np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))\n",
        "\n",
        "def knn_predict(train_data, train_labels, x_val, k):\n",
        "    similarities = []\n",
        "    for i in range(len(train_data)):\n",
        "        sim = cosine_similarity(x_val, train_data[i])\n",
        "        similarities.append((sim, train_labels[i]))\n",
        "\n",
        "    similarities = sorted(similarities, reverse=True)[:k]\n",
        "    labels = [label for _, label in similarities]\n",
        "\n",
        "    return max(set(labels), key=labels.count)\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/spambase.csv\")\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "data_array = data.to_numpy()\n",
        "\n",
        "train_size = int(0.8 * len(data_array))\n",
        "train_data, test_data = data_array[:train_size, :-1], data_array[train_size:, :-1]\n",
        "train_labels, test_labels = data_array[:train_size, -1], data_array[train_size:, -1]\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "avg_train_accuracy = 0\n",
        "avg_val_accuracy = 0\n",
        "\n",
        "for train_idx, val_idx in kf.split(train_data):\n",
        "    sub_train_data, sub_val_data = train_data[train_idx], train_data[val_idx]\n",
        "    sub_train_labels, sub_val_labels = train_labels[train_idx], train_labels[val_idx]\n",
        "\n",
        "    k = 5\n",
        "    y_val_pred = []\n",
        "    for x_val in sub_val_data:\n",
        "        pred_label = knn_predict(sub_train_data, sub_train_labels, x_val, k)\n",
        "        y_val_pred.append(pred_label)\n",
        "\n",
        "    val_accuracy = np.mean(np.array(y_val_pred) == sub_val_labels)\n",
        "    avg_val_accuracy += val_accuracy\n",
        "\n",
        "    spam_proportion = y_val_pred.count(1) / len(y_val_pred)\n",
        "\n",
        "avg_val_accuracy /= kf.get_n_splits()\n",
        "\n",
        "y_test_pred = []\n",
        "for x_test in test_data:\n",
        "    pred_label = knn_predict(train_data, train_labels, x_test, k)\n",
        "    y_test_pred.append(pred_label)\n",
        "\n",
        "test_accuracy = np.mean(np.array(y_test_pred) == test_labels)\n",
        "\n",
        "print(\"Validation Dataset Accuracy:\", avg_val_accuracy)\n",
        "print(\"Test Dataset Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Te-q0n8WoQh",
        "outputId": "106a681c-8cda-4b6c-c16d-0bedf35e6101"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Dataset Accuracy: 0.8345108695652174\n",
            "Test Dataset Accuracy: 0.8175895765472313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LR\n",
        "\n",
        "1. model learning: You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "    \n",
        "    $y = sigmoid(MX)$\n",
        "\n",
        "step 1: add one more column (all value is 1) in X -> X' = np.c_[np.ones((len(X), 1)), X]\n",
        "\n",
        "step 2:vector M = np.random.randn(len(X[0])+1, 1);\n",
        "\n",
        "key formula for step 3 (Note: n is the size of the TRAINING dataset; $cdot$ is dot production ):\n",
        "\n",
        "1. $pred_y = sigmoid(M\\cdot X')$\n",
        "\n",
        "2. $loss = -\\sum(y\\cdot log(pred_y)+(1-y)\\cdot log(1-pred_y))/n$\n",
        "\n",
        "3. $gm=X'\\cdot (pred_y - y)*2/n$\n",
        "\n",
        "Step 3 example code:\n",
        "   ```\n",
        "   #Step 3: performing gradient descent on whole dataset:\n",
        "   best_model = M\n",
        "   best_performace = 0\n",
        "   for i in range(epoch):\n",
        "     pred_y = ...\n",
        "     gm = ...\n",
        "     _p = performace(model, val)\n",
        "     if _p > best_performance:\n",
        "        best_model = M\n",
        "        best_performance = _p\n",
        "     M = M - learning_rate*gm\n",
        "   ```\n",
        "\n",
        "2. model evaluation(on val dataset):\n",
        "  \n",
        "   calculate pred_y, if more than 0.5, then the predicted label is 1."
      ],
      "metadata": {
        "id": "OUzUupva0Fxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##THIS IS JUST TO CHECK THE CONTENTS OF THE DATABASE\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/spambase.csv\")\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "print(data.columns)\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9T2qXeacDD9",
        "outputId": "5a996214-221f-4d6e-ebba-e77f525a5c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d',\n",
            "       'word_freq_our', 'word_freq_over', 'word_freq_remove',\n",
            "       'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
            "       'word_freq_receive', 'word_freq_will', 'word_freq_people',\n",
            "       'word_freq_report', 'word_freq_addresses', 'word_freq_free',\n",
            "       'word_freq_business', 'word_freq_email', 'word_freq_you',\n",
            "       'word_freq_credit', 'word_freq_your', 'word_freq_font', 'word_freq_000',\n",
            "       'word_freq_money', 'word_freq_hp', 'word_freq_hpl', 'word_freq_george',\n",
            "       'word_freq_650', 'word_freq_lab', 'word_freq_labs', 'word_freq_telnet',\n",
            "       'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85',\n",
            "       'word_freq_technology', 'word_freq_1999', 'word_freq_parts',\n",
            "       'word_freq_pm', 'word_freq_direct', 'word_freq_cs', 'word_freq_meeting',\n",
            "       'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
            "       'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
            "       'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!',\n",
            "       'char_freq_$', 'char_freq_#', 'capital_run_length_average',\n",
            "       'capital_run_length_longest', 'capital_run_length_total', 'spam'],\n",
            "      dtype='object')\n",
            "4601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def sigmoid(z):\n",
        "    z = np.clip(z, -500, 500)\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def add_intercept(X):\n",
        "    return np.c_[np.ones((len(X), 1)), X]\n",
        "\n",
        "def compute_loss_gradient(X, y, M):\n",
        "    pred_y = sigmoid(np.dot(X, M))\n",
        "    pred_y = np.clip(pred_y, 1e-15, 1 - 1e-15)\n",
        "    loss = -np.sum(y * np.log(pred_y) + (1 - y) * np.log(1 - pred_y)) / len(X)\n",
        "    gradient = np.dot(X.T, (pred_y - y)) * 2 / len(X)\n",
        "    return loss, gradient\n",
        "\n",
        "def evaluate_model(X, y, M):\n",
        "    pred_y = sigmoid(np.dot(X, M))\n",
        "    pred_y_binary = (pred_y > 0.5).astype(int)\n",
        "    accuracy = np.mean(pred_y_binary.squeeze() == y.squeeze())\n",
        "    return accuracy\n",
        "\n",
        "def train_model(X_train, y_train, X_val, y_val, epoch, learning_rate):\n",
        "    M = np.random.randn(X_train.shape[1], 1)\n",
        "    best_model = M\n",
        "    best_performance = 0\n",
        "\n",
        "    for i in range(epoch):\n",
        "        loss, gradient = compute_loss_gradient(X_train, y_train, M)\n",
        "        accuracy = evaluate_model(X_val, y_val, M)\n",
        "\n",
        "        if accuracy > best_performance:\n",
        "            best_model = M\n",
        "            best_performance = accuracy\n",
        "\n",
        "        M = M - learning_rate * gradient\n",
        "\n",
        "    return best_model, best_performance\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/ColabNotebooks/spambase.csv\")\n",
        "\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "train_size = int(0.8 * len(data))\n",
        "train_data, test_data = data[:train_size], data[train_size:]\n",
        "\n",
        "fold5 = KFold(n_splits=5)\n",
        "\n",
        "test_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for train_idx, val_idx in fold5.split(train_data):\n",
        "    sub_train = train_data.iloc[train_idx]\n",
        "    sub_val = train_data.iloc[val_idx]\n",
        "    X_train = sub_train.iloc[:, :-1].values\n",
        "    y_train = sub_train.iloc[:, -1].values.reshape(-1, 1)\n",
        "    X_val = sub_val.iloc[:, :-1].values\n",
        "    y_val = sub_val.iloc[:, -1].values.reshape(-1, 1)\n",
        "    X_train_prime = add_intercept(X_train)\n",
        "    X_val_prime = add_intercept(X_val)\n",
        "    best_model, val_accuracy = train_model(X_train_prime, y_train, X_val_prime, y_val, epoch=3000, learning_rate=0.01)\n",
        "\n",
        "    X_test = test_data.iloc[:, :-1].values\n",
        "    y_test = test_data.iloc[:, -1].values.reshape(-1, 1)\n",
        "    X_test_prime = add_intercept(X_test)\n",
        "    test_accuracy = evaluate_model(X_test_prime, y_test, best_model)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "print(\"Test accuracies:\", test_accuracies)\n",
        "print(\"Validation accuracies:\", val_accuracies)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydeom14DYfPA",
        "outputId": "b2a8aa8c-9c4c-4dc9-944d-efefc125f647"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracies: [0.7937024972855592, 0.7785016286644951, 0.747014115092291, 0.7676438653637351, 0.7806731813246471]\n",
            "Validation accuracies: [0.8097826086956522, 0.8070652173913043, 0.7717391304347826, 0.7785326086956522, 0.8029891304347826]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html"
      ],
      "metadata": {
        "id": "mAssSW_I0GvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def performance(model, x, y):\n",
        "    predictions = model.predict(x)\n",
        "    accuracy = accuracy_score(y, predictions)\n",
        "    print('Accuracy:', accuracy)\n",
        "    return accuracy\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "performance(model, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "e0MQ0eo0MnmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acbe5e10-3fee-4535-8707-f78b28e92432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    }
  ]
}